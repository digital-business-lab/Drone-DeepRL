{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments.EnvRandomReturn import EnvRandomReturn\n",
    "from environments.EnvSimpleReturn import EnvSimpleReturn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" \n",
    "    The QNetwork should output Q-values for each discrete action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the forward pass of the network \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World-Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(state_dim + action_dim, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc_state = nn.Linear(hidden_size, state_dim)\n",
    "        self.fc_reward = nn.Linear(hidden_size, 1)\n",
    "        self.fc_terminated = nn.Linear(hidden_size, 1)\n",
    "        self.fc_truncated = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, observation, action, hidden_state=None):\n",
    "        x = torch.cat([observation, action], dim=-1)  # (batch, seq_len, input_dim)\n",
    "\n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)  # lstm_out: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Compute outputs\n",
    "        predicted_state = self.fc_state(lstm_out)\n",
    "        predicted_reward = torch.tanh(self.fc_reward(lstm_out))\n",
    "        predicted_terminated = torch.sigmoid(self.fc_terminated(lstm_out))\n",
    "        predicted_truncated = torch.sigmoid(self.fc_truncated(lstm_out))\n",
    "\n",
    "        return predicted_state, predicted_reward, predicted_terminated, predicted_truncated, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, seq_len):\n",
    "        self.capacity = capacity\n",
    "        self.seq_len = seq_len  # Length of sequences to sample\n",
    "        self.buffer = deque(maxlen=capacity)  # Use deque for efficiency\n",
    "\n",
    "    def add(self, state, action, reward, next_state, terminated, truncated):\n",
    "        \"\"\"Stores a transition in the replay buffer.\"\"\"\n",
    "        # TODO: Maybe make it so we can filter good and bad episodes\n",
    "        self.buffer.append((state, action, reward, next_state, terminated, truncated))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a batch of sequence-length transitions, prioritizing recent data.\"\"\"\n",
    "        if len(self.buffer) < self.seq_len:\n",
    "            return None  # Ensure enough data\n",
    "\n",
    "        recent_fraction = 0.3  # Adjust this to control how much of the buffer is recent\n",
    "        recent_size = max(self.seq_len, int(len(self.buffer) * recent_fraction))  \n",
    "        max_index = len(self.buffer) - recent_size  # Start sampling from this index\n",
    "\n",
    "        # Select valid starting indices with preference for recent transitions\n",
    "        indices = np.random.choice(range(max_index, len(self.buffer) - self.seq_len), batch_size, replace=False)\n",
    "\n",
    "        sequences = []\n",
    "        for idx in indices:\n",
    "            seq = list(self.buffer)[idx : idx + self.seq_len]  # Get a sequence\n",
    "            states, actions, rewards, next_states, terminates, truncates = zip(*seq)\n",
    "\n",
    "            sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n",
    "                            torch.tensor(actions, dtype=torch.long, device=DEVICE),  \n",
    "                            torch.tensor(rewards, dtype=torch.float32, device=DEVICE).unsqueeze(-1),\n",
    "                            torch.tensor(next_states, dtype=torch.float32, device=DEVICE),\n",
    "                            torch.tensor(terminates, dtype=torch.bool, device=DEVICE).unsqueeze(-1),\n",
    "                            torch.tensor(truncates, dtype=torch.bool, device=DEVICE).unsqueeze(-1)\n",
    "                            ))\n",
    "\n",
    "        # Convert list of sequences into batch tensors\n",
    "        batch = tuple(torch.stack(b, dim=0) for b in zip(*sequences))  \n",
    "        return batch  # Shapes: (batch_size, seq_len, feature_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# ENV = EnvRandomReturn()\n",
    "ENV = EnvSimpleReturn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Environment (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART_ENV = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "CAPACITY_BUFFER = 10_000\n",
    "OBS_SPACE_SIZE = 5\n",
    "ACTION_SIZE = 6\n",
    "SEQ_LEN = 32\n",
    "SIMULATED_UPDATES = 32\n",
    "IMAGINARY_HORIZON = 10\n",
    "TRAIN_WORLD_MODEL_EVERY = 10\n",
    "EPOCHS = 30\n",
    "\n",
    "# Initialize components\n",
    "Q_NETWORK = QNetwork(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE)\n",
    "WORLD_MODEL = WorldModel(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE).to(DEVICE)\n",
    "REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "IMAGINARY_REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_q_network = torch.optim.Adam(Q_NETWORK.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_world_model = torch.optim.Adam(WORLD_MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize hidden state\n",
    "h = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "c = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "hidden_state_real = (h, c)\n",
    "# last_real_hidden_state = hidden_state_real\n",
    "\n",
    "# Initialize loss functions\n",
    "world_model_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length REPLAY_BUFFER 10000/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas Graf\\AppData\\Local\\Temp\\ipykernel_9768\\297240850.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0; AVERAGE WORLD_MODEL LOSS: 2866.5469\n",
      "EPOCH: 1; AVERAGE WORLD_MODEL LOSS: 2827.6597\n",
      "EPOCH: 2; AVERAGE WORLD_MODEL LOSS: 2827.5578\n",
      "EPOCH: 3; AVERAGE WORLD_MODEL LOSS: 2820.0902\n",
      "EPOCH: 4; AVERAGE WORLD_MODEL LOSS: 2803.6474\n",
      "EPOCH: 5; AVERAGE WORLD_MODEL LOSS: 2824.8251\n",
      "EPOCH: 6; AVERAGE WORLD_MODEL LOSS: 2818.9834\n",
      "EPOCH: 7; AVERAGE WORLD_MODEL LOSS: 2832.7511\n",
      "EPOCH: 8; AVERAGE WORLD_MODEL LOSS: 2813.1847\n",
      "EPOCH: 9; AVERAGE WORLD_MODEL LOSS: 2821.5357\n",
      "EPOCH: 10; AVERAGE WORLD_MODEL LOSS: 2816.0117\n",
      "EPOCH: 11; AVERAGE WORLD_MODEL LOSS: 2819.9783\n",
      "EPOCH: 12; AVERAGE WORLD_MODEL LOSS: 2814.1318\n",
      "EPOCH: 13; AVERAGE WORLD_MODEL LOSS: 2829.1576\n",
      "EPOCH: 14; AVERAGE WORLD_MODEL LOSS: 2814.0491\n"
     ]
    }
   ],
   "source": [
    "SAFE_ACTIONS = [1, 3, 4, 5]  # Define safe actions\n",
    "UNSAFE_ACTIONS = [1, 2]  # Define unsafe actions\n",
    "\n",
    "\n",
    "t = 0\n",
    "while t <= REPLAY_BUFFER.capacity:\n",
    "    state, _info = ENV.reset()# ENV.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        \"\"\" \n",
    "        Only for the pretraining of the WORLDMODEL before entering the main training loop\n",
    "        make the randomness of up and down lesser so we get longer episodes and the model\n",
    "        learns the dependencies between states and its rewards better aswell as logner episodes.\n",
    "        \"\"\"\n",
    "        # action: int = random.choice(range(ACTION_SIZE))\n",
    "        if random.random() < 0.9:  # 70% chance to pick a safe action\n",
    "            action = random.choice(SAFE_ACTIONS)  \n",
    "        else:\n",
    "            action = random.choice(UNSAFE_ACTIONS)  # 30% chance for altitude changes\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=ACTION_SIZE\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, terminated, truncated, _info = ENV.step(action=action)# ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            terminated, # bool\n",
    "            truncated # bool\n",
    "        )\n",
    "       \n",
    "        state = next_state\n",
    "\n",
    "        # Update idx\n",
    "        t += 1\n",
    "\n",
    "        if t % 10_000 == 0:\n",
    "            print(f\"Length REPLAY_BUFFER {t}/{REPLAY_BUFFER.capacity}\")\n",
    "\n",
    "\n",
    "#----Train world model----#\n",
    "for epoch in range(EPOCHS):\n",
    "    num_samples = len(REPLAY_BUFFER)  # Total transitions in buffer\n",
    "    num_batches = num_samples // BATCH_SIZE  # Total batches per epoch\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _ in range(num_batches):  \n",
    "        batch = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "        if batch is None:  # Skip if buffer is too small\n",
    "            continue\n",
    "\n",
    "        states, actions, rewards, next_states, terminates, truncates = batch\n",
    "\n",
    "        # Forward pass\n",
    "        pred_states, pred_rewards, pred_terminates, pred_truncates, hidden_state_real = WORLD_MODEL(\n",
    "            states, actions, hidden_state_real\n",
    "        )\n",
    "\n",
    "        # Detach hidden state between sequences to prevent gradient leakage\n",
    "        hidden_state_real = tuple([h.detach() for h in hidden_state_real])  \n",
    "\n",
    "        # Compute loss\n",
    "        state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "        reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "        terminated_loss = world_model_loss_fn(pred_terminates, terminates.float())\n",
    "        truncated_loss = world_model_loss_fn(pred_truncates, truncates.float())\n",
    "        total_loss = state_loss + reward_loss + terminated_loss + truncated_loss\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_world_model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer_world_model.step()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"EPOCH: {epoch}; AVERAGE WORLD_MODEL LOSS: {avg_epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV = EnvRandomReturn()\n",
    "# TODO: Test code on cartpole environment\n",
    "for episode in range(EPISODES):\n",
    "    state, _info = ENV.reset()# Only get observation\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Epsilon greedy policy\n",
    "        if random.random() < EPSILON:\n",
    "            action: int = random.choice(range(ACTION_SIZE)) # Exploration\n",
    "        else:\n",
    "            q_values = Q_NETWORK(torch.tensor(state, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item() # Exploitation\n",
    "\n",
    "        # Transform action len=1 to one_hot len=6 for WorldModle input\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=ACTION_SIZE\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, terminated, truncated, _info = ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            terminated, # bool\n",
    "            truncated # bool\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay Epsilon\n",
    "    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    #----------Train WorldModel using real data----------#\n",
    "    if len(REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN and episode % TRAIN_WORLD_MODEL_EVERY == 0:\n",
    "        # Sample real data from REPLAY_BUFFER\n",
    "        (states, actions, rewards, next_states, terminates, truncates) = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        # Write params to the DEVICE for GPU support\n",
    "        states = states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        rewards = rewards.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        terminates = terminates.to(DEVICE)\n",
    "        truncates = truncates.to(DEVICE)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        pred_states, pred_rewards, pred_terminates, pred_truncates, hidden_state_real = WORLD_MODEL(\n",
    "            states, actions, hidden_state_real\n",
    "        )\n",
    "        hidden_state_real = tuple([h.detach() for h in hidden_state_real])\n",
    "\n",
    "        # Compute losses\n",
    "        state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "        reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "        terminated_loss = world_model_loss_fn(pred_terminates, terminates.float())\n",
    "        truncated_loss = world_model_loss_fn(pred_truncates, truncates.float())\n",
    "        total_loss = state_loss + reward_loss + terminated_loss + truncated_loss\n",
    "\n",
    "        print(f\"EPISODE: {episode}; WORLD_MODEL LOSS: {total_loss}\")\n",
    "\n",
    "        optimizer_world_model.zero_grad()\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer_world_model.step()\n",
    "\n",
    "        # Update last_real_hidden_state for LSTM Learning\n",
    "        last_real_hidden_state = hidden_state_real\n",
    "\n",
    "\n",
    "    # ---- Use World Model as a simulator to train the Q-Network ----\n",
    "    for idx, _ in enumerate(range(SIMULATED_UPDATES)):  # Train Q-network multiple times using synthetic experience\n",
    "        state_sim = torch.tensor(ENV.reset()[0], dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)  # 1 Batch, 1 Seq, 5 elements\n",
    "\n",
    "        # Intitialize hidden state sim from last real hidden state (num_layers, BATCH, hidden_size)\n",
    "        hidden_state_sim = (\n",
    "            last_real_hidden_state[0][:, 0:1, :].detach(),  # Shape: (num_layers, 1, hidden_size)\n",
    "            last_real_hidden_state[1][:, 0:1, :].detach()\n",
    "        )\n",
    "\n",
    "        for _ in range(IMAGINARY_HORIZON):\n",
    "            # Choose action using current Q-network\n",
    "            q_values = Q_NETWORK(state_sim)\n",
    "            action_sim = torch.argmax(q_values, dim=-1).item()\n",
    "\n",
    "            # Convert action to one-hot encoding\n",
    "            action_one_hot_sim = torch.nn.functional.one_hot(\n",
    "                torch.tensor(action_sim, dtype=torch.long, device=DEVICE), num_classes=ACTION_SIZE\n",
    "            ).unsqueeze(0).unsqueeze(0)\n",
    " \n",
    "            # Predict next state, reward, and done using the World Model\n",
    "            predicted_next_state, predicted_reward, predicted_terminated, predicted_truncated, hidden_state_sim = WORLD_MODEL(\n",
    "                state_sim, action_one_hot_sim, hidden_state_sim\n",
    "            )\n",
    "\n",
    "            # Store synthetic experience\n",
    "            # Remove dimensionalitys which were needed for LSTM to store in buffer\n",
    "            IMAGINARY_REPLAY_BUFFER.add(\n",
    "                state_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                predicted_reward.squeeze().item(),\n",
    "                predicted_next_state.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                predicted_terminated.squeeze().item(),\n",
    "                predicted_truncated.squeeze().item()\n",
    "            )\n",
    "\n",
    "\n",
    "            # Ensure termination if certain 90%\n",
    "            if predicted_terminated.item() > 0.9 or predicted_truncated.item() > 0.9:\n",
    "                break\n",
    "            \n",
    "            state_sim = predicted_next_state.detach()  # Move to predicted next state\n",
    "\n",
    "    # ---- Train Q-Network on synthetic experience ----\n",
    "    if len(IMAGINARY_REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        # Train from real and synthetic experiences\n",
    "        real_sample_size = int(BATCH_SIZE * 0.5)  # Half from real data\n",
    "        sim_sample_size = BATCH_SIZE - real_sample_size  # Half from simulated\n",
    "\n",
    "        real_experiences = REPLAY_BUFFER.sample(real_sample_size)\n",
    "        simulated_experiences = IMAGINARY_REPLAY_BUFFER.sample(sim_sample_size)\n",
    "\n",
    "        # Combine real and simulated experiences\n",
    "        states_sim = torch.cat([real_experiences[0], simulated_experiences[0]], dim=0).to(DEVICE)\n",
    "        actions_sim = torch.cat([real_experiences[1], simulated_experiences[1]], dim=0).to(DEVICE)\n",
    "        rewards_sim = torch.cat([real_experiences[2], simulated_experiences[2]], dim=0).to(DEVICE)\n",
    "        next_states_sim = torch.cat([real_experiences[3], simulated_experiences[3]], dim=0).to(DEVICE)\n",
    "        terminated_sim = torch.cat([real_experiences[4], simulated_experiences[4]], dim=0).to(DEVICE).float()\n",
    "        truncated_sim = torch.cat([real_experiences[5], simulated_experiences[5]], dim=0).to(DEVICE).float()\n",
    "\n",
    "        # Get Q-values\n",
    "        q_values = Q_NETWORK(states_sim)  \n",
    "        actions_sim_indices = actions_sim.argmax(dim=-1).unsqueeze(-1)# .view(64, 32, 1)  # Shape [64, 32, 1]\n",
    "\n",
    "        # Now we gather the Q-values corresponding to the actions\n",
    "        q_value = q_values.gather(2, actions_sim_indices)\n",
    "        \n",
    "        # Compute target using Bellman equation\n",
    "        next_q_values = Q_NETWORK(next_states_sim).max(-1)[0].unsqueeze(-1)  # Max Q-value for next state\n",
    "        target = rewards_sim + GAMMA * next_q_values * (1 - terminated_sim)  # Bellman update\n",
    "\n",
    "        # Compute loss\n",
    "        loss_q = torch.nn.MSELoss()(q_value, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_q_network.zero_grad()\n",
    "        loss_q.backward()\n",
    "        optimizer_q_network.step()\n",
    "\n",
    "        print(f\"Q-Network Loss: {loss_q.item():.4f}\")\n",
    "\n",
    "\n",
    "# Save models\n",
    "torch.save(WORLD_MODEL.state_dict(), \"world_model.pth\")\n",
    "torch.save(Q_NETWORK.state_dict(), \"dqn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
