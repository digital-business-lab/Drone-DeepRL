{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments.EnvRandomReturn import EnvRandomReturn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" \n",
    "    The QNetwork should output Q-values for each discrete action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the forward pass of the network \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World-Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(state_dim + action_dim, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc_state = nn.Linear(hidden_size, state_dim)\n",
    "        self.fc_reward = nn.Linear(hidden_size, 1)\n",
    "        self.fc_done = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, observation, action, hidden_state=None):\n",
    "        x = torch.cat([observation, action], dim=-1)  # (batch, seq_len, input_dim)\n",
    "\n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)  # lstm_out: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Compute outputs\n",
    "        predicted_state = self.fc_state(lstm_out)\n",
    "        predicted_reward = torch.tanh(self.fc_reward(lstm_out))\n",
    "        predicted_done = torch.sigmoid(self.fc_done(lstm_out))\n",
    "\n",
    "        return predicted_state, predicted_reward, predicted_done, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, seq_len):\n",
    "        self.capacity = capacity\n",
    "        self.seq_len = seq_len  # Length of sequences to sample\n",
    "        self.buffer = deque(maxlen=capacity)  # Use deque for efficiency\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores a transition in the replay buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a batch of sequence-length transitions.\"\"\"\n",
    "        if len(self.buffer) < self.seq_len:  # Ensure enough data\n",
    "            return None\n",
    "\n",
    "        # Select valid starting indices\n",
    "        indices = np.random.choice(len(self.buffer) - self.seq_len, batch_size, replace=False)\n",
    "\n",
    "        sequences = []\n",
    "        for idx in indices:\n",
    "            seq = list(self.buffer)[idx : idx + self.seq_len]  # Get a sequence\n",
    "            states, actions, rewards, next_states, dones = zip(*seq)\n",
    "\n",
    "            sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n",
    "                              # Long for dicrete space\n",
    "                              torch.tensor(actions, dtype=torch.long, device=DEVICE),\n",
    "                              torch.tensor(rewards, dtype=torch.float32, device=DEVICE).unsqueeze(-1),\n",
    "                              torch.tensor(next_states, dtype=torch.float32, device=DEVICE),\n",
    "                              torch.tensor(dones, dtype=torch.bool, device=DEVICE).unsqueeze(-1)))\n",
    "\n",
    "        # Convert list of sequences into batch tensors\n",
    "        batch = tuple(torch.stack(b, dim=0) for b in zip(*sequences))  \n",
    "        return batch  # Shapes: (batch_size, seq_len, feature_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "CAPACITY_BUFFER = 10_000\n",
    "OBS_SPACE_SIZE = 5\n",
    "ACTION_SIZE = 6\n",
    "SEQ_LEN = 32\n",
    "SIMULATED_UPDATES = 32\n",
    "IMAGINARY_HORIZON = 10\n",
    "\n",
    "# Initialize components\n",
    "Q_NETWORK = QNetwork(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE)\n",
    "WORLD_MODEL = WorldModel(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE).to(DEVICE)\n",
    "REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "IMAGINARY_REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_q_network = torch.optim.Adam(Q_NETWORK.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_world_model = torch.optim.Adam(WORLD_MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize hidden state\n",
    "h = torch.zeros(WORLD_MODEL.num_layers, 1, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "c = torch.zeros(WORLD_MODEL.num_layers, 1, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "hidden_state = (h, c)\n",
    "\n",
    "# Initialize loss functions\n",
    "world_model_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "ENV = EnvRandomReturn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas Graf\\AppData\\Local\\Temp\\ipykernel_17372\\2130668505.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n",
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 32, 6])) that is different to the input size (torch.Size([64, 32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Network Loss: 0.0245\n",
      "Q-Network Loss: 0.0074\n",
      "Q-Network Loss: 0.0019\n",
      "Q-Network Loss: 0.0020\n",
      "Q-Network Loss: 0.0024\n",
      "Q-Network Loss: 0.0029\n",
      "Q-Network Loss: 0.0034\n",
      "Q-Network Loss: 0.0038\n",
      "Q-Network Loss: 0.0035\n",
      "EPISODE: 9; WORLD_MODEL LOSS: 2294.375\n",
      "Q-Network Loss: 0.0047\n",
      "EPISODE: 10; WORLD_MODEL LOSS: 2289.241943359375\n",
      "Q-Network Loss: 0.0040\n",
      "EPISODE: 11; WORLD_MODEL LOSS: 2284.106689453125\n",
      "Q-Network Loss: 0.0022\n",
      "EPISODE: 12; WORLD_MODEL LOSS: 2279.398193359375\n",
      "Q-Network Loss: 0.0032\n",
      "EPISODE: 13; WORLD_MODEL LOSS: 3549.140869140625\n",
      "Q-Network Loss: 0.0028\n",
      "EPISODE: 14; WORLD_MODEL LOSS: 3470.896484375\n",
      "Q-Network Loss: 0.0035\n",
      "EPISODE: 15; WORLD_MODEL LOSS: 3465.9599609375\n",
      "Q-Network Loss: 0.0037\n",
      "EPISODE: 16; WORLD_MODEL LOSS: 3466.772705078125\n",
      "Q-Network Loss: 0.0049\n",
      "EPISODE: 17; WORLD_MODEL LOSS: 3357.652099609375\n",
      "Q-Network Loss: 0.0060\n",
      "EPISODE: 18; WORLD_MODEL LOSS: 3363.933837890625\n",
      "Q-Network Loss: 0.0067\n",
      "EPISODE: 19; WORLD_MODEL LOSS: 3958.60302734375\n",
      "Q-Network Loss: 0.0115\n",
      "EPISODE: 20; WORLD_MODEL LOSS: 4129.50341796875\n",
      "Q-Network Loss: 0.0097\n",
      "EPISODE: 21; WORLD_MODEL LOSS: 4144.62890625\n",
      "Q-Network Loss: 0.0176\n",
      "EPISODE: 22; WORLD_MODEL LOSS: 3736.148681640625\n",
      "Q-Network Loss: 0.0187\n",
      "EPISODE: 23; WORLD_MODEL LOSS: 3859.608154296875\n",
      "Q-Network Loss: 0.0233\n",
      "EPISODE: 24; WORLD_MODEL LOSS: 3594.7578125\n",
      "Q-Network Loss: 0.0554\n",
      "EPISODE: 25; WORLD_MODEL LOSS: 3734.284912109375\n",
      "Q-Network Loss: 0.0772\n",
      "EPISODE: 26; WORLD_MODEL LOSS: 4276.064453125\n",
      "Q-Network Loss: 0.0535\n",
      "EPISODE: 27; WORLD_MODEL LOSS: 3581.13720703125\n",
      "Q-Network Loss: 0.0782\n",
      "EPISODE: 28; WORLD_MODEL LOSS: 3693.77099609375\n",
      "Q-Network Loss: 0.0760\n",
      "EPISODE: 29; WORLD_MODEL LOSS: 4327.3154296875\n",
      "Q-Network Loss: 0.0999\n",
      "EPISODE: 30; WORLD_MODEL LOSS: 5673.52197265625\n",
      "Q-Network Loss: 0.0895\n",
      "EPISODE: 31; WORLD_MODEL LOSS: 5958.45458984375\n",
      "Q-Network Loss: 0.1160\n",
      "EPISODE: 32; WORLD_MODEL LOSS: 6294.4306640625\n",
      "Q-Network Loss: 0.1335\n",
      "EPISODE: 33; WORLD_MODEL LOSS: 5432.318359375\n",
      "Q-Network Loss: 0.1492\n",
      "EPISODE: 34; WORLD_MODEL LOSS: 6296.4970703125\n",
      "Q-Network Loss: 0.1613\n",
      "EPISODE: 35; WORLD_MODEL LOSS: 4710.765625\n",
      "Q-Network Loss: 0.1770\n",
      "EPISODE: 36; WORLD_MODEL LOSS: 4550.041015625\n",
      "Q-Network Loss: 0.1907\n",
      "EPISODE: 37; WORLD_MODEL LOSS: 4684.880859375\n",
      "Q-Network Loss: 0.2013\n",
      "EPISODE: 38; WORLD_MODEL LOSS: 4072.549560546875\n",
      "Q-Network Loss: 0.2116\n",
      "EPISODE: 39; WORLD_MODEL LOSS: 4743.22119140625\n",
      "Q-Network Loss: 0.2242\n",
      "EPISODE: 40; WORLD_MODEL LOSS: 4784.2568359375\n",
      "Q-Network Loss: 0.2299\n",
      "EPISODE: 41; WORLD_MODEL LOSS: 5807.890625\n",
      "Q-Network Loss: 0.2340\n",
      "EPISODE: 42; WORLD_MODEL LOSS: 5021.07763671875\n",
      "Q-Network Loss: 0.2370\n",
      "EPISODE: 43; WORLD_MODEL LOSS: 4560.33984375\n",
      "Q-Network Loss: 0.2356\n",
      "EPISODE: 44; WORLD_MODEL LOSS: 3656.648681640625\n",
      "Q-Network Loss: 0.2289\n",
      "EPISODE: 45; WORLD_MODEL LOSS: 4149.75146484375\n",
      "Q-Network Loss: 0.2244\n",
      "EPISODE: 46; WORLD_MODEL LOSS: 3550.911376953125\n",
      "Q-Network Loss: 0.2200\n",
      "EPISODE: 47; WORLD_MODEL LOSS: 6493.5322265625\n",
      "Q-Network Loss: 0.2122\n",
      "EPISODE: 48; WORLD_MODEL LOSS: 6464.7431640625\n",
      "Q-Network Loss: 0.2043\n",
      "EPISODE: 49; WORLD_MODEL LOSS: 6495.9755859375\n",
      "Q-Network Loss: 0.1971\n",
      "EPISODE: 50; WORLD_MODEL LOSS: 6311.41845703125\n",
      "Q-Network Loss: 0.1845\n",
      "EPISODE: 51; WORLD_MODEL LOSS: 6311.19873046875\n",
      "Q-Network Loss: 0.1755\n",
      "EPISODE: 52; WORLD_MODEL LOSS: 6414.19189453125\n",
      "Q-Network Loss: 0.1699\n",
      "EPISODE: 53; WORLD_MODEL LOSS: 6339.431640625\n",
      "Q-Network Loss: 0.1602\n",
      "EPISODE: 54; WORLD_MODEL LOSS: 6117.3525390625\n",
      "Q-Network Loss: 0.1547\n",
      "EPISODE: 55; WORLD_MODEL LOSS: 3785.98291015625\n",
      "Q-Network Loss: 0.1453\n",
      "EPISODE: 56; WORLD_MODEL LOSS: 3840.23095703125\n",
      "Q-Network Loss: 0.1334\n",
      "EPISODE: 57; WORLD_MODEL LOSS: 3729.964599609375\n",
      "Q-Network Loss: 0.1133\n",
      "EPISODE: 58; WORLD_MODEL LOSS: 4954.20263671875\n",
      "Q-Network Loss: 0.0917\n",
      "EPISODE: 59; WORLD_MODEL LOSS: 5054.67626953125\n",
      "Q-Network Loss: 0.0721\n",
      "EPISODE: 60; WORLD_MODEL LOSS: 5079.88427734375\n",
      "Q-Network Loss: 0.0533\n",
      "EPISODE: 61; WORLD_MODEL LOSS: 4777.0087890625\n",
      "Q-Network Loss: 0.0377\n",
      "EPISODE: 62; WORLD_MODEL LOSS: 5302.26708984375\n",
      "Q-Network Loss: 0.0285\n",
      "EPISODE: 63; WORLD_MODEL LOSS: 5494.86181640625\n",
      "Q-Network Loss: 0.0233\n",
      "EPISODE: 64; WORLD_MODEL LOSS: 5196.91259765625\n",
      "Q-Network Loss: 0.0228\n",
      "EPISODE: 65; WORLD_MODEL LOSS: 4094.24951171875\n",
      "Q-Network Loss: 0.0251\n",
      "EPISODE: 66; WORLD_MODEL LOSS: 7125.203125\n",
      "Q-Network Loss: 0.0286\n",
      "EPISODE: 67; WORLD_MODEL LOSS: 5535.8134765625\n",
      "Q-Network Loss: 0.0334\n",
      "EPISODE: 68; WORLD_MODEL LOSS: 5541.85986328125\n",
      "Q-Network Loss: 0.0333\n",
      "EPISODE: 69; WORLD_MODEL LOSS: 6542.2333984375\n",
      "Q-Network Loss: 0.0322\n",
      "EPISODE: 70; WORLD_MODEL LOSS: 7097.82568359375\n",
      "Q-Network Loss: 0.0314\n",
      "EPISODE: 71; WORLD_MODEL LOSS: 4583.74755859375\n",
      "Q-Network Loss: 0.0265\n",
      "EPISODE: 72; WORLD_MODEL LOSS: 5978.48095703125\n",
      "Q-Network Loss: 0.0253\n",
      "EPISODE: 73; WORLD_MODEL LOSS: 5895.81494140625\n",
      "Q-Network Loss: 0.0218\n",
      "EPISODE: 74; WORLD_MODEL LOSS: 6323.88134765625\n",
      "Q-Network Loss: 0.0182\n",
      "EPISODE: 75; WORLD_MODEL LOSS: 6014.6357421875\n",
      "Q-Network Loss: 0.0144\n",
      "EPISODE: 76; WORLD_MODEL LOSS: 6093.7666015625\n",
      "Q-Network Loss: 0.0119\n",
      "EPISODE: 77; WORLD_MODEL LOSS: 4526.443359375\n",
      "Q-Network Loss: 0.0097\n",
      "EPISODE: 78; WORLD_MODEL LOSS: 5195.3876953125\n",
      "Q-Network Loss: 0.0086\n",
      "EPISODE: 79; WORLD_MODEL LOSS: 6493.533203125\n",
      "Q-Network Loss: 0.0088\n",
      "EPISODE: 80; WORLD_MODEL LOSS: 6493.42919921875\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# ---- Use World Model as a simulator to train the Q-Network ----\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(SIMULATED_UPDATES)):  \u001b[38;5;66;03m# Train Q-network multiple times using synthetic experience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     state_sim = torch.tensor(\u001b[43mENV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m], dtype=torch.float32, device=DEVICE).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# 1 Batch, 1 Seq, 5 elements\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Initialize hidden states for LSTM\u001b[39;00m\n\u001b[32m     79\u001b[39m     h_0 = torch.zeros(WORLD_MODEL.lstm.num_layers, \u001b[32m1\u001b[39m, WORLD_MODEL.lstm.hidden_size, device=DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\src\\environments\\EnvRandomReturn.py:153\u001b[39m, in \u001b[36mEnvRandomReturn.reset\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m    146\u001b[39m p.resetBasePositionAndOrientation(\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.drone_id,\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.drone_initial_pos, \n\u001b[32m    149\u001b[39m     \u001b[38;5;28mself\u001b[39m.drone_initial_ori\n\u001b[32m    150\u001b[39m )\n\u001b[32m    152\u001b[39m p.removeBody(\u001b[38;5;28mself\u001b[39m.obj_id_a)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremoveBody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj_id_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m.target_a, \u001b[38;5;28mself\u001b[39m.target_b = \u001b[38;5;28mself\u001b[39m.create_checkpoints()\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.current_target = \u001b[38;5;28mself\u001b[39m.target_b\n",
      "\u001b[31merror\u001b[39m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    state, _info = ENV.reset()# Only get observation\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon greedy policy\n",
    "        if random.random() < EPSILON:\n",
    "            # Exploration\n",
    "            action: int = random.choice(range(6))\n",
    "        else:\n",
    "            # Exploitation\n",
    "            q_values = Q_NETWORK(torch.tensor(state, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        # Transform action len=1 to one_hot len=6 for WorldModle input\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=6\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, done, _ = ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            done # bool\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay Epsilon\n",
    "    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    #----------Train WorldModel using real data----------#\n",
    "    if len(REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        (states, actions, rewards, next_states, dones) = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        states = states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        rewards = rewards.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        dones = dones.to(DEVICE)\n",
    "\n",
    "        optimizer_world_model.zero_grad()\n",
    "\n",
    "        h_0 = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "        c_0 = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "        hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_states, pred_rewards, pred_dones, _ = WORLD_MODEL(\n",
    "            states, actions, hidden_state\n",
    "        )\n",
    "\n",
    "        # Compute losses\n",
    "        state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "        reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "        done_loss = world_model_loss_fn(pred_dones, dones.float())\n",
    "        total_loss = state_loss + reward_loss + done_loss\n",
    "\n",
    "        print(f\"EPISODE: {episode}; WORLD_MODEL LOSS: {total_loss}\")\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer_world_model.step()\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Use World Model as a simulator to train the Q-Network ----\n",
    "    for idx, _ in enumerate(range(SIMULATED_UPDATES)):  # Train Q-network multiple times using synthetic experience\n",
    "        state_sim = torch.tensor(ENV.reset()[0], dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)  # 1 Batch, 1 Seq, 5 elements\n",
    "\n",
    "        # Initialize hidden states for LSTM\n",
    "        h_0 = torch.zeros(WORLD_MODEL.lstm.num_layers, 1, WORLD_MODEL.lstm.hidden_size, device=DEVICE)\n",
    "        c_0 = torch.zeros(WORLD_MODEL.lstm.num_layers, 1, WORLD_MODEL.lstm.hidden_size, device=DEVICE)\n",
    "        hidden_state = (h_0, c_0)\n",
    "\n",
    "        for _ in range(IMAGINARY_HORIZON):  # Generate synthetic rollouts\n",
    "            # Choose action using current Q-network\n",
    "            q_values = Q_NETWORK(state_sim)\n",
    "            action_sim = torch.argmax(q_values, dim=-1).item()\n",
    "            # Convert action to one-hot encoding\n",
    "            action_one_hot_sim = torch.nn.functional.one_hot(\n",
    "                torch.tensor(action_sim, dtype=torch.long, device=DEVICE), num_classes=6\n",
    "            ).unsqueeze(0).unsqueeze(0)\n",
    " \n",
    "            # Predict next state, reward, and done using the World Model\n",
    "            predicted_next_state, predicted_reward, predicted_done, hidden_state = WORLD_MODEL(\n",
    "                state_sim, action_one_hot_sim, hidden_state\n",
    "            )\n",
    "            # print(action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy())\n",
    "            # Store synthetic experience\n",
    "            IMAGINARY_REPLAY_BUFFER.add(\n",
    "                state_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Remove batch dims and convert to numpy array\n",
    "                action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Same for action\n",
    "                predicted_reward.squeeze().item(),  # Squeeze to remove extra dimension and get scalar\n",
    "                predicted_next_state.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Squeeze and detach for next_state\n",
    "                predicted_done.squeeze().item()  # Squeeze and convert to scalar\n",
    "            )\n",
    "\n",
    "\n",
    "            if predicted_done.item() > 0.5:  # Terminate if World Model predicts episode end\n",
    "                break\n",
    "            \n",
    "            state_sim = predicted_next_state.detach()  # Move to predicted next state\n",
    "\n",
    "    # ---- Train Q-Network on synthetic experience ----\n",
    "    if len(IMAGINARY_REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        (states_sim, actions_sim, rewards_sim, next_states_sim, dones_sim) = IMAGINARY_REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        # Move to device\n",
    "        states_sim = states_sim.to(DEVICE)\n",
    "        actions_sim = actions_sim.to(DEVICE).long()  # Ensure correct type\n",
    "        rewards_sim = rewards_sim.to(DEVICE)\n",
    "        next_states_sim = next_states_sim.to(DEVICE)\n",
    "        dones_sim = dones_sim.to(DEVICE).float()  # Convert to float\n",
    "\n",
    "        # Get Q-values\n",
    "        q_values = Q_NETWORK(states_sim)  \n",
    "        actions_sim_indices = actions_sim.argmax(dim=-1).view(64, 32, 1)  # Shape [64, 32, 1]\n",
    "        # Now we gather the Q-values corresponding to the actions\n",
    "        q_value = q_values.gather(2, actions_sim_indices)\n",
    "        \n",
    "        # Compute target using Bellman equation\n",
    "        next_q_values = Q_NETWORK(next_states_sim).max(1)[0].unsqueeze(1)  # Max Q-value for next state\n",
    "        target = rewards_sim + GAMMA * next_q_values * (1 - dones_sim)  # Bellman update\n",
    "\n",
    "        # Compute loss\n",
    "        loss_q = torch.nn.MSELoss()(q_value, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_q_network.zero_grad()\n",
    "        loss_q.backward()\n",
    "        optimizer_q_network.step()\n",
    "\n",
    "        print(f\"Q-Network Loss: {loss_q.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
