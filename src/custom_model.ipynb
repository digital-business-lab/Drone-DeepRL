{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments.EnvRandomReturn import EnvRandomReturn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" \n",
    "    The QNetwork should output Q-values for each discrete action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the forward pass of the network \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World-Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(state_dim + action_dim, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc_state = nn.Linear(hidden_size, state_dim)\n",
    "        self.fc_reward = nn.Linear(hidden_size, 1)\n",
    "        self.fc_done = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, observation, action, hidden_state=None):\n",
    "        x = torch.cat([observation, action], dim=-1)  # (batch, seq_len, input_dim)\n",
    "\n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)  # lstm_out: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Compute outputs\n",
    "        predicted_state = self.fc_state(lstm_out)\n",
    "        predicted_reward = torch.tanh(self.fc_reward(lstm_out))\n",
    "        predicted_done = torch.sigmoid(self.fc_done(lstm_out))\n",
    "\n",
    "        return predicted_state, predicted_reward, predicted_done, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, seq_len):\n",
    "        self.capacity = capacity\n",
    "        self.seq_len = seq_len  # Length of sequences to sample\n",
    "        self.buffer = deque(maxlen=capacity)  # Use deque for efficiency\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores a transition in the replay buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a batch of sequence-length transitions.\"\"\"\n",
    "        if len(self.buffer) < self.seq_len:  # Ensure enough data\n",
    "            return None\n",
    "\n",
    "        # Select valid starting indices\n",
    "        indices = np.random.choice(len(self.buffer) - self.seq_len, batch_size, replace=False)\n",
    "\n",
    "        sequences = []\n",
    "        for idx in indices:\n",
    "            seq = list(self.buffer)[idx : idx + self.seq_len]  # Get a sequence\n",
    "            states, actions, rewards, next_states, dones = zip(*seq)\n",
    "\n",
    "            sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n",
    "                              # Long for dicrete space\n",
    "                              torch.tensor(actions, dtype=torch.long, device=DEVICE),\n",
    "                              torch.tensor(rewards, dtype=torch.float32, device=DEVICE).unsqueeze(-1),\n",
    "                              torch.tensor(next_states, dtype=torch.float32, device=DEVICE),\n",
    "                              torch.tensor(dones, dtype=torch.bool, device=DEVICE).unsqueeze(-1)))\n",
    "\n",
    "        # Convert list of sequences into batch tensors\n",
    "        batch = tuple(torch.stack(b, dim=0) for b in zip(*sequences))  \n",
    "        return batch  # Shapes: (batch_size, seq_len, feature_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "CAPACITY_BUFFER = 10_000\n",
    "OBS_SPACE_SIZE = 5\n",
    "ACTION_SIZE = 6\n",
    "SEQ_LEN = 32\n",
    "SIMULATED_UPDATES = 32\n",
    "IMAGINARY_HORIZON = 10\n",
    "\n",
    "# Initialize components\n",
    "Q_NETWORK = QNetwork(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE)\n",
    "WORLD_MODEL = WorldModel(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE).to(DEVICE)\n",
    "REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "IMAGINARY_REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_q_network = torch.optim.Adam(Q_NETWORK.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_world_model = torch.optim.Adam(WORLD_MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize hidden state\n",
    "h = torch.zeros(WORLD_MODEL.num_layers, 1, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "c = torch.zeros(WORLD_MODEL.num_layers, 1, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "hidden_state = (h, c)\n",
    "\n",
    "# Initialize loss functions\n",
    "world_model_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "ENV = EnvRandomReturn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas Graf\\AppData\\Local\\Temp\\ipykernel_17048\\2130668505.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0; WORLD_MODEL LOSS: 636.6249389648438\n",
      "EPISODE: 1; WORLD_MODEL LOSS: 800.5535278320312\n",
      "EPISODE: 2; WORLD_MODEL LOSS: 853.1513061523438\n",
      "Q-Network Loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([64, 32, 6])) that is different to the input size (torch.Size([64, 32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 3; WORLD_MODEL LOSS: 866.3450927734375\n",
      "Q-Network Loss: 0.0068\n",
      "EPISODE: 4; WORLD_MODEL LOSS: 850.7408447265625\n",
      "Q-Network Loss: 0.0144\n",
      "EPISODE: 5; WORLD_MODEL LOSS: 840.7267456054688\n",
      "Q-Network Loss: 0.0184\n",
      "EPISODE: 6; WORLD_MODEL LOSS: 920.8984375\n",
      "Q-Network Loss: 0.0119\n",
      "EPISODE: 7; WORLD_MODEL LOSS: 875.2791748046875\n",
      "Q-Network Loss: 0.0055\n",
      "EPISODE: 8; WORLD_MODEL LOSS: 964.0557861328125\n",
      "Q-Network Loss: 0.0062\n",
      "EPISODE: 9; WORLD_MODEL LOSS: 1044.994384765625\n",
      "Q-Network Loss: 0.0090\n",
      "EPISODE: 10; WORLD_MODEL LOSS: 903.0107421875\n",
      "Q-Network Loss: 0.0094\n",
      "EPISODE: 11; WORLD_MODEL LOSS: 996.0928955078125\n",
      "Q-Network Loss: 0.0091\n",
      "EPISODE: 12; WORLD_MODEL LOSS: 947.3857421875\n",
      "Q-Network Loss: 0.0072\n",
      "EPISODE: 13; WORLD_MODEL LOSS: 898.9411010742188\n",
      "Q-Network Loss: 0.0081\n",
      "EPISODE: 14; WORLD_MODEL LOSS: 1042.9952392578125\n",
      "Q-Network Loss: 0.0065\n",
      "EPISODE: 15; WORLD_MODEL LOSS: 920.8938598632812\n",
      "Q-Network Loss: 0.0075\n",
      "EPISODE: 16; WORLD_MODEL LOSS: 1075.9970703125\n",
      "Q-Network Loss: 0.0089\n",
      "EPISODE: 17; WORLD_MODEL LOSS: 913.1345825195312\n",
      "Q-Network Loss: 0.0172\n",
      "EPISODE: 18; WORLD_MODEL LOSS: 1103.48828125\n",
      "Q-Network Loss: 0.0170\n",
      "EPISODE: 19; WORLD_MODEL LOSS: 1013.4530639648438\n",
      "Q-Network Loss: 0.0163\n",
      "EPISODE: 20; WORLD_MODEL LOSS: 1023.4197387695312\n",
      "Q-Network Loss: 0.0121\n",
      "EPISODE: 21; WORLD_MODEL LOSS: 1097.9937744140625\n",
      "Q-Network Loss: 0.0122\n",
      "EPISODE: 22; WORLD_MODEL LOSS: 1023.4564819335938\n",
      "Q-Network Loss: 0.1981\n",
      "EPISODE: 23; WORLD_MODEL LOSS: 867.0405883789062\n",
      "Q-Network Loss: 0.2840\n",
      "EPISODE: 24; WORLD_MODEL LOSS: 867.5741577148438\n",
      "Q-Network Loss: 0.3180\n",
      "EPISODE: 25; WORLD_MODEL LOSS: 815.2587890625\n",
      "Q-Network Loss: 0.4619\n",
      "EPISODE: 26; WORLD_MODEL LOSS: 883.3027954101562\n",
      "Q-Network Loss: 0.4879\n",
      "EPISODE: 27; WORLD_MODEL LOSS: 1026.87646484375\n",
      "Q-Network Loss: 0.5439\n",
      "EPISODE: 28; WORLD_MODEL LOSS: 922.7188110351562\n",
      "Q-Network Loss: 0.5606\n",
      "EPISODE: 29; WORLD_MODEL LOSS: 850.0879516601562\n",
      "Q-Network Loss: 0.5417\n",
      "EPISODE: 30; WORLD_MODEL LOSS: 951.198486328125\n",
      "Q-Network Loss: 0.5551\n",
      "EPISODE: 31; WORLD_MODEL LOSS: 1003.1512451171875\n",
      "Q-Network Loss: 0.5533\n",
      "EPISODE: 32; WORLD_MODEL LOSS: 949.5384521484375\n",
      "Q-Network Loss: 0.5486\n",
      "EPISODE: 33; WORLD_MODEL LOSS: 885.6135864257812\n",
      "Q-Network Loss: 0.5341\n",
      "EPISODE: 34; WORLD_MODEL LOSS: 858.4129028320312\n",
      "Q-Network Loss: 0.5295\n",
      "EPISODE: 35; WORLD_MODEL LOSS: 1005.6249389648438\n",
      "Q-Network Loss: 0.5732\n",
      "EPISODE: 36; WORLD_MODEL LOSS: 837.7920532226562\n",
      "Q-Network Loss: 0.5203\n",
      "EPISODE: 37; WORLD_MODEL LOSS: 889.4415893554688\n",
      "Q-Network Loss: 0.5318\n",
      "EPISODE: 38; WORLD_MODEL LOSS: 868.4365234375\n",
      "Q-Network Loss: 0.5354\n",
      "EPISODE: 39; WORLD_MODEL LOSS: 906.7467041015625\n",
      "Q-Network Loss: 0.4472\n",
      "EPISODE: 40; WORLD_MODEL LOSS: 867.3004150390625\n",
      "Q-Network Loss: 0.4911\n",
      "EPISODE: 41; WORLD_MODEL LOSS: 868.912841796875\n",
      "Q-Network Loss: 0.4784\n",
      "EPISODE: 42; WORLD_MODEL LOSS: 814.7266845703125\n",
      "Q-Network Loss: 0.4315\n",
      "EPISODE: 43; WORLD_MODEL LOSS: 971.481689453125\n",
      "Q-Network Loss: 0.4074\n",
      "EPISODE: 44; WORLD_MODEL LOSS: 864.2578735351562\n",
      "Q-Network Loss: 0.4196\n",
      "EPISODE: 45; WORLD_MODEL LOSS: 791.7807006835938\n",
      "Q-Network Loss: 0.3922\n",
      "EPISODE: 46; WORLD_MODEL LOSS: 1025.3331298828125\n",
      "Q-Network Loss: 0.3658\n",
      "EPISODE: 47; WORLD_MODEL LOSS: 837.277099609375\n",
      "Q-Network Loss: 0.3512\n",
      "EPISODE: 48; WORLD_MODEL LOSS: 875.8851928710938\n",
      "Q-Network Loss: 0.3239\n",
      "EPISODE: 49; WORLD_MODEL LOSS: 869.5555419921875\n",
      "Q-Network Loss: 0.3144\n",
      "EPISODE: 50; WORLD_MODEL LOSS: 902.4575805664062\n",
      "Q-Network Loss: 0.3055\n",
      "EPISODE: 51; WORLD_MODEL LOSS: 945.5044555664062\n",
      "Q-Network Loss: 0.2863\n",
      "EPISODE: 52; WORLD_MODEL LOSS: 879.1953125\n",
      "Q-Network Loss: 0.2729\n",
      "EPISODE: 53; WORLD_MODEL LOSS: 1013.7341918945312\n",
      "Q-Network Loss: 0.2506\n",
      "EPISODE: 54; WORLD_MODEL LOSS: 801.736572265625\n",
      "Q-Network Loss: 0.2418\n",
      "EPISODE: 55; WORLD_MODEL LOSS: 866.0949096679688\n",
      "Q-Network Loss: 0.2388\n",
      "EPISODE: 56; WORLD_MODEL LOSS: 930.0595092773438\n",
      "Q-Network Loss: 0.2121\n",
      "EPISODE: 57; WORLD_MODEL LOSS: 799.1826782226562\n",
      "Q-Network Loss: 0.1952\n",
      "EPISODE: 58; WORLD_MODEL LOSS: 900.8861083984375\n",
      "Q-Network Loss: 0.1805\n",
      "EPISODE: 59; WORLD_MODEL LOSS: 1017.30908203125\n",
      "Q-Network Loss: 0.1591\n",
      "EPISODE: 60; WORLD_MODEL LOSS: 869.4579467773438\n",
      "Q-Network Loss: 0.1453\n",
      "EPISODE: 61; WORLD_MODEL LOSS: 887.5032348632812\n",
      "Q-Network Loss: 0.1314\n",
      "EPISODE: 62; WORLD_MODEL LOSS: 869.1919555664062\n",
      "Q-Network Loss: 0.1133\n",
      "EPISODE: 63; WORLD_MODEL LOSS: 880.42578125\n",
      "Q-Network Loss: 0.0953\n",
      "EPISODE: 64; WORLD_MODEL LOSS: 941.9986572265625\n",
      "Q-Network Loss: 0.0826\n",
      "EPISODE: 65; WORLD_MODEL LOSS: 897.3486328125\n",
      "Q-Network Loss: 0.0771\n",
      "EPISODE: 66; WORLD_MODEL LOSS: 939.5763549804688\n",
      "Q-Network Loss: 0.0708\n",
      "EPISODE: 67; WORLD_MODEL LOSS: 1003.6900634765625\n",
      "Q-Network Loss: 0.0665\n",
      "EPISODE: 68; WORLD_MODEL LOSS: 909.5936889648438\n",
      "Q-Network Loss: 0.0595\n",
      "EPISODE: 69; WORLD_MODEL LOSS: 902.883544921875\n",
      "Q-Network Loss: 0.0562\n",
      "EPISODE: 70; WORLD_MODEL LOSS: 849.7871704101562\n",
      "Q-Network Loss: 0.0499\n",
      "EPISODE: 71; WORLD_MODEL LOSS: 837.2789306640625\n",
      "Q-Network Loss: 0.0474\n",
      "EPISODE: 72; WORLD_MODEL LOSS: 890.8455200195312\n",
      "Q-Network Loss: 0.0435\n",
      "EPISODE: 73; WORLD_MODEL LOSS: 1086.858642578125\n",
      "Q-Network Loss: 0.0387\n",
      "EPISODE: 74; WORLD_MODEL LOSS: 948.4165649414062\n",
      "Q-Network Loss: 0.0344\n",
      "EPISODE: 75; WORLD_MODEL LOSS: 785.7009887695312\n",
      "Q-Network Loss: 0.0302\n",
      "EPISODE: 76; WORLD_MODEL LOSS: 914.3824462890625\n",
      "Q-Network Loss: 0.0283\n",
      "EPISODE: 77; WORLD_MODEL LOSS: 916.1619873046875\n",
      "Q-Network Loss: 0.0278\n",
      "EPISODE: 78; WORLD_MODEL LOSS: 888.7515258789062\n",
      "Q-Network Loss: 0.0265\n",
      "EPISODE: 79; WORLD_MODEL LOSS: 805.5836181640625\n",
      "Q-Network Loss: 0.0247\n",
      "EPISODE: 80; WORLD_MODEL LOSS: 833.1563720703125\n",
      "Q-Network Loss: 0.0227\n",
      "EPISODE: 81; WORLD_MODEL LOSS: 950.3359985351562\n",
      "Q-Network Loss: 0.0211\n",
      "EPISODE: 82; WORLD_MODEL LOSS: 924.8812866210938\n",
      "Q-Network Loss: 0.0195\n",
      "EPISODE: 83; WORLD_MODEL LOSS: 947.3400268554688\n",
      "Q-Network Loss: 0.0174\n",
      "EPISODE: 84; WORLD_MODEL LOSS: 800.4751586914062\n",
      "Q-Network Loss: 0.0168\n",
      "EPISODE: 85; WORLD_MODEL LOSS: 1018.2981567382812\n",
      "Q-Network Loss: 0.0153\n",
      "EPISODE: 86; WORLD_MODEL LOSS: 973.8382568359375\n",
      "Q-Network Loss: 0.0152\n",
      "EPISODE: 87; WORLD_MODEL LOSS: 992.6752319335938\n",
      "Q-Network Loss: 0.0157\n",
      "EPISODE: 88; WORLD_MODEL LOSS: 987.7662963867188\n",
      "Q-Network Loss: 0.0138\n",
      "EPISODE: 89; WORLD_MODEL LOSS: 960.4611206054688\n",
      "Q-Network Loss: 0.0123\n",
      "EPISODE: 90; WORLD_MODEL LOSS: 983.577392578125\n",
      "Q-Network Loss: 0.0123\n",
      "EPISODE: 91; WORLD_MODEL LOSS: 1027.23095703125\n",
      "Q-Network Loss: 0.0119\n",
      "EPISODE: 92; WORLD_MODEL LOSS: 927.8854370117188\n",
      "Q-Network Loss: 0.0113\n",
      "EPISODE: 93; WORLD_MODEL LOSS: 867.9071044921875\n",
      "Q-Network Loss: 0.0104\n",
      "EPISODE: 94; WORLD_MODEL LOSS: 928.6917114257812\n",
      "Q-Network Loss: 0.0098\n",
      "EPISODE: 95; WORLD_MODEL LOSS: 1044.976318359375\n",
      "Q-Network Loss: 0.0093\n",
      "EPISODE: 96; WORLD_MODEL LOSS: 980.8742065429688\n",
      "Q-Network Loss: 0.0087\n",
      "EPISODE: 97; WORLD_MODEL LOSS: 1067.281982421875\n",
      "Q-Network Loss: 0.0079\n",
      "EPISODE: 98; WORLD_MODEL LOSS: 1061.5841064453125\n",
      "Q-Network Loss: 0.0072\n",
      "EPISODE: 99; WORLD_MODEL LOSS: 968.740478515625\n",
      "Q-Network Loss: 0.0067\n",
      "EPISODE: 100; WORLD_MODEL LOSS: 940.120849609375\n",
      "Q-Network Loss: 0.0064\n",
      "EPISODE: 101; WORLD_MODEL LOSS: 917.6591796875\n",
      "Q-Network Loss: 0.0061\n",
      "EPISODE: 102; WORLD_MODEL LOSS: 912.671875\n",
      "Q-Network Loss: 0.0057\n",
      "EPISODE: 103; WORLD_MODEL LOSS: 984.9424438476562\n",
      "Q-Network Loss: 0.0054\n",
      "EPISODE: 104; WORLD_MODEL LOSS: 984.6809692382812\n",
      "Q-Network Loss: 0.0051\n",
      "EPISODE: 105; WORLD_MODEL LOSS: 948.4701538085938\n",
      "Q-Network Loss: 0.0047\n",
      "EPISODE: 106; WORLD_MODEL LOSS: 1049.163330078125\n",
      "Q-Network Loss: 0.0041\n",
      "EPISODE: 107; WORLD_MODEL LOSS: 952.3556518554688\n",
      "Q-Network Loss: 0.0034\n",
      "EPISODE: 108; WORLD_MODEL LOSS: 986.4821166992188\n",
      "Q-Network Loss: 0.0031\n",
      "EPISODE: 109; WORLD_MODEL LOSS: 1082.3167724609375\n",
      "Q-Network Loss: 0.0031\n",
      "EPISODE: 110; WORLD_MODEL LOSS: 1114.6820068359375\n",
      "Q-Network Loss: 0.0031\n",
      "EPISODE: 111; WORLD_MODEL LOSS: 1025.453369140625\n",
      "Q-Network Loss: 0.0030\n",
      "EPISODE: 112; WORLD_MODEL LOSS: 934.1372680664062\n",
      "Q-Network Loss: 0.0029\n",
      "EPISODE: 113; WORLD_MODEL LOSS: 1039.3675537109375\n",
      "Q-Network Loss: 0.0028\n",
      "EPISODE: 114; WORLD_MODEL LOSS: 1211.6143798828125\n",
      "Q-Network Loss: 0.0024\n",
      "EPISODE: 115; WORLD_MODEL LOSS: 1179.371826171875\n",
      "Q-Network Loss: 0.0020\n",
      "EPISODE: 116; WORLD_MODEL LOSS: 1074.1341552734375\n",
      "Q-Network Loss: 0.0017\n",
      "EPISODE: 117; WORLD_MODEL LOSS: 868.0037841796875\n",
      "Q-Network Loss: 0.0016\n",
      "EPISODE: 118; WORLD_MODEL LOSS: 957.3124389648438\n",
      "Q-Network Loss: 0.0015\n",
      "EPISODE: 119; WORLD_MODEL LOSS: 999.0968627929688\n",
      "Q-Network Loss: 0.0015\n",
      "EPISODE: 120; WORLD_MODEL LOSS: 835.3367309570312\n",
      "Q-Network Loss: 0.0016\n",
      "EPISODE: 121; WORLD_MODEL LOSS: 1074.5860595703125\n",
      "Q-Network Loss: 0.0014\n",
      "EPISODE: 122; WORLD_MODEL LOSS: 1270.8612060546875\n",
      "Q-Network Loss: 0.0010\n",
      "EPISODE: 123; WORLD_MODEL LOSS: 1129.635986328125\n",
      "Q-Network Loss: 0.0008\n",
      "EPISODE: 124; WORLD_MODEL LOSS: 1056.984130859375\n",
      "Q-Network Loss: 0.0009\n",
      "EPISODE: 125; WORLD_MODEL LOSS: 1035.437255859375\n",
      "Q-Network Loss: 0.0009\n",
      "EPISODE: 126; WORLD_MODEL LOSS: 1003.9737548828125\n",
      "Q-Network Loss: 0.0009\n",
      "EPISODE: 127; WORLD_MODEL LOSS: 1046.2301025390625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     88\u001b[39m action_one_hot_sim = torch.nn.functional.one_hot(\n\u001b[32m     89\u001b[39m     torch.tensor(action_sim, dtype=torch.long, device=DEVICE), num_classes=\u001b[32m6\u001b[39m\n\u001b[32m     90\u001b[39m ).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Predict next state, reward, and done using the World Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m predicted_next_state, predicted_reward, predicted_done, hidden_state = \u001b[43mWORLD_MODEL\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_one_hot_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# print(action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy())\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Store synthetic experience\u001b[39;00m\n\u001b[32m     98\u001b[39m IMAGINARY_REPLAY_BUFFER.add(\n\u001b[32m     99\u001b[39m     state_sim.squeeze(\u001b[32m0\u001b[39m).squeeze(\u001b[32m0\u001b[39m).detach().cpu().numpy(),  \u001b[38;5;66;03m# Remove batch dims and convert to numpy array\u001b[39;00m\n\u001b[32m    100\u001b[39m     action_one_hot_sim.squeeze(\u001b[32m0\u001b[39m).squeeze(\u001b[32m0\u001b[39m).detach().cpu().numpy(),  \u001b[38;5;66;03m# Same for action\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     predicted_done.squeeze().item()  \u001b[38;5;66;03m# Squeeze and convert to scalar\u001b[39;00m\n\u001b[32m    104\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mWorldModel.forward\u001b[39m\u001b[34m(self, observation, action, hidden_state)\u001b[39m\n\u001b[32m     26\u001b[39m lstm_out, hidden_state = \u001b[38;5;28mself\u001b[39m.lstm(x, hidden_state)  \u001b[38;5;66;03m# lstm_out: (batch, seq_len, hidden_size)\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Compute outputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m predicted_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m predicted_reward = torch.tanh(\u001b[38;5;28mself\u001b[39m.fc_reward(lstm_out))\n\u001b[32m     31\u001b[39m predicted_done = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.fc_done(lstm_out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    state, _info = ENV.reset()# Only get observation\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon greedy policy\n",
    "        if random.random() < EPSILON:\n",
    "            # Exploration\n",
    "            action: int = random.choice(range(6))\n",
    "        else:\n",
    "            # Exploitation\n",
    "            q_values = Q_NETWORK(torch.tensor(state, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        # Transform action len=1 to one_hot len=6 for WorldModle input\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=6\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, done, _ = ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            done # bool\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay Epsilon\n",
    "    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    #----------Train WorldModel using real data----------#\n",
    "    if len(REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        (states, actions, rewards, next_states, dones) = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        states = states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        rewards = rewards.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        dones = dones.to(DEVICE)\n",
    "\n",
    "        optimizer_world_model.zero_grad()\n",
    "\n",
    "        h_0 = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "        c_0 = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "        hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_states, pred_rewards, pred_dones, _ = WORLD_MODEL(\n",
    "            states, actions, hidden_state\n",
    "        )\n",
    "\n",
    "        # Compute losses\n",
    "        state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "        reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "        done_loss = world_model_loss_fn(pred_dones, dones.float())\n",
    "        total_loss = state_loss + reward_loss + done_loss\n",
    "\n",
    "        print(f\"EPISODE: {episode}; WORLD_MODEL LOSS: {total_loss}\")\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer_world_model.step()\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Use World Model as a simulator to train the Q-Network ----\n",
    "    for idx, _ in enumerate(range(SIMULATED_UPDATES)):  # Train Q-network multiple times using synthetic experience\n",
    "        state_sim = torch.tensor(ENV.reset()[0], dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)  # 1 Batch, 1 Seq, 5 elements\n",
    "\n",
    "        # Initialize hidden states for LSTM\n",
    "        h_0 = torch.zeros(WORLD_MODEL.lstm.num_layers, 1, WORLD_MODEL.lstm.hidden_size, device=DEVICE)\n",
    "        c_0 = torch.zeros(WORLD_MODEL.lstm.num_layers, 1, WORLD_MODEL.lstm.hidden_size, device=DEVICE)\n",
    "        hidden_state = (h_0, c_0)\n",
    "\n",
    "        for _ in range(IMAGINARY_HORIZON):  # Generate synthetic rollouts\n",
    "            # Choose action using current Q-network\n",
    "            q_values = Q_NETWORK(state_sim)\n",
    "            action_sim = torch.argmax(q_values, dim=-1).item()\n",
    "            # Convert action to one-hot encoding\n",
    "            action_one_hot_sim = torch.nn.functional.one_hot(\n",
    "                torch.tensor(action_sim, dtype=torch.long, device=DEVICE), num_classes=6\n",
    "            ).unsqueeze(0).unsqueeze(0)\n",
    " \n",
    "            # Predict next state, reward, and done using the World Model\n",
    "            predicted_next_state, predicted_reward, predicted_done, hidden_state = WORLD_MODEL(\n",
    "                state_sim, action_one_hot_sim, hidden_state\n",
    "            )\n",
    "            # print(action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy())\n",
    "            # Store synthetic experience\n",
    "            IMAGINARY_REPLAY_BUFFER.add(\n",
    "                state_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Remove batch dims and convert to numpy array\n",
    "                action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Same for action\n",
    "                predicted_reward.squeeze().item(),  # Squeeze to remove extra dimension and get scalar\n",
    "                predicted_next_state.squeeze(0).squeeze(0).detach().cpu().numpy(),  # Squeeze and detach for next_state\n",
    "                predicted_done.squeeze().item()  # Squeeze and convert to scalar\n",
    "            )\n",
    "\n",
    "\n",
    "            if predicted_done.item() > 0.5:  # Terminate if World Model predicts episode end\n",
    "                break\n",
    "            \n",
    "            state_sim = predicted_next_state.detach()  # Move to predicted next state\n",
    "\n",
    "    # ---- Train Q-Network on synthetic experience ----\n",
    "    if len(IMAGINARY_REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        (states_sim, actions_sim, rewards_sim, next_states_sim, dones_sim) = IMAGINARY_REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        # Move to device\n",
    "        states_sim = states_sim.to(DEVICE)\n",
    "        actions_sim = actions_sim.to(DEVICE).long()  # Ensure correct type\n",
    "        rewards_sim = rewards_sim.to(DEVICE)\n",
    "        next_states_sim = next_states_sim.to(DEVICE)\n",
    "        dones_sim = dones_sim.to(DEVICE).float()  # Convert to float\n",
    "\n",
    "        # Get Q-values\n",
    "        q_values = Q_NETWORK(states_sim)  \n",
    "        actions_sim_indices = actions_sim.argmax(dim=-1).view(64, 32, 1)  # Shape [64, 32, 1]\n",
    "        # Now we gather the Q-values corresponding to the actions\n",
    "        q_value = q_values.gather(2, actions_sim_indices)\n",
    "        \n",
    "        # Compute target using Bellman equation\n",
    "        next_q_values = Q_NETWORK(next_states_sim).max(1)[0].unsqueeze(1)  # Max Q-value for next state\n",
    "        target = rewards_sim + GAMMA * next_q_values * (1 - dones_sim)  # Bellman update\n",
    "\n",
    "        # Compute loss\n",
    "        loss_q = torch.nn.MSELoss()(q_value, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_q_network.zero_grad()\n",
    "        loss_q.backward()\n",
    "        optimizer_q_network.step()\n",
    "\n",
    "        print(f\"Q-Network Loss: {loss_q.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
