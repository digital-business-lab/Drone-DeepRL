{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments.EnvRandomReturn import EnvRandomReturn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\" \n",
    "    The QNetwork should output Q-values for each discrete action.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Defines the forward pass of the network \"\"\"\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World-Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(state_dim + action_dim, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Output layers\n",
    "        self.fc_state = nn.Linear(hidden_size, state_dim)\n",
    "        self.fc_reward = nn.Linear(hidden_size, 1)\n",
    "        self.fc_done = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, observation, action, hidden_state=None):\n",
    "        x = torch.cat([observation, action], dim=-1)  # (batch, seq_len, input_dim)\n",
    "\n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "            hidden_state = (h_0, c_0)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)  # lstm_out: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Compute outputs\n",
    "        predicted_state = self.fc_state(lstm_out)\n",
    "        predicted_reward = torch.tanh(self.fc_reward(lstm_out))\n",
    "        predicted_done = torch.sigmoid(self.fc_done(lstm_out))\n",
    "\n",
    "        return predicted_state, predicted_reward, predicted_done, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, seq_len):\n",
    "        self.capacity = capacity\n",
    "        self.seq_len = seq_len  # Length of sequences to sample\n",
    "        self.buffer = deque(maxlen=capacity)  # Use deque for efficiency\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores a transition in the replay buffer.\"\"\"\n",
    "        # TODO: Maybe make it so we can filter good and bad episodes\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a batch of sequence-length transitions, prioritizing recent data.\"\"\"\n",
    "        if len(self.buffer) < self.seq_len:\n",
    "            return None  # Ensure enough data\n",
    "\n",
    "        recent_fraction = 0.3  # Adjust this to control how much of the buffer is recent\n",
    "        recent_size = max(self.seq_len, int(len(self.buffer) * recent_fraction))  \n",
    "        max_index = len(self.buffer) - recent_size  # Start sampling from this index\n",
    "\n",
    "        # Select valid starting indices with preference for recent transitions\n",
    "        indices = np.random.choice(range(max_index, len(self.buffer) - self.seq_len), batch_size, replace=False)\n",
    "\n",
    "        sequences = []\n",
    "        for idx in indices:\n",
    "            seq = list(self.buffer)[idx : idx + self.seq_len]  # Get a sequence\n",
    "            states, actions, rewards, next_states, dones = zip(*seq)\n",
    "\n",
    "            sequences.append((torch.tensor(states, dtype=torch.float32, device=DEVICE),\n",
    "                            torch.tensor(actions, dtype=torch.long, device=DEVICE),  \n",
    "                            torch.tensor(rewards, dtype=torch.float32, device=DEVICE).unsqueeze(-1),\n",
    "                            torch.tensor(next_states, dtype=torch.float32, device=DEVICE),\n",
    "                            torch.tensor(dones, dtype=torch.bool, device=DEVICE).unsqueeze(-1)))\n",
    "\n",
    "        # Convert list of sequences into batch tensors\n",
    "        batch = tuple(torch.stack(b, dim=0) for b in zip(*sequences))  \n",
    "        return batch  # Shapes: (batch_size, seq_len, feature_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git_Repos\\Drone-DeepRL\\venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "ENV = EnvRandomReturn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Environment (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART_ENV = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "CAPACITY_BUFFER = 100_000\n",
    "OBS_SPACE_SIZE = 5\n",
    "ACTION_SIZE = 6\n",
    "SEQ_LEN = 32\n",
    "SIMULATED_UPDATES = 32\n",
    "IMAGINARY_HORIZON = 10\n",
    "TRAIN_WORLD_MODEL_EVERY = 10\n",
    "EPOCHS = 30\n",
    "\n",
    "# Initialize components\n",
    "Q_NETWORK = QNetwork(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE)\n",
    "WORLD_MODEL = WorldModel(state_dim=OBS_SPACE_SIZE, action_dim=ACTION_SIZE).to(DEVICE)\n",
    "REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "IMAGINARY_REPLAY_BUFFER = ReplayBuffer(capacity=CAPACITY_BUFFER, seq_len=SEQ_LEN)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_q_network = torch.optim.Adam(Q_NETWORK.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_world_model = torch.optim.Adam(WORLD_MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Initialize hidden state\n",
    "h = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "c = torch.zeros(WORLD_MODEL.num_layers, BATCH_SIZE, WORLD_MODEL.hidden_size).to(DEVICE)\n",
    "hidden_state_real = (h, c)\n",
    "# last_real_hidden_state = hidden_state_real\n",
    "\n",
    "# Initialize loss functions\n",
    "world_model_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m action_one_hot: np.ndarray = action_one_hot.numpy()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Step in real environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m next_state, reward, done, _info = \u001b[43mENV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# ENV.step(action)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Apply to memory of current episode\u001b[39;00m\n\u001b[32m     24\u001b[39m REPLAY_BUFFER.add(\n\u001b[32m     25\u001b[39m     state, \u001b[38;5;66;03m# np.ndarray\u001b[39;00m\n\u001b[32m     26\u001b[39m     action_one_hot, \u001b[38;5;66;03m# np.ndarray\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     done \u001b[38;5;66;03m# bool\u001b[39;00m\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\src\\environments\\EnvRandomReturn.py:138\u001b[39m, in \u001b[36mEnvRandomReturn.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    135\u001b[39m p.stepSimulation()\n\u001b[32m    136\u001b[39m time.sleep(\u001b[32m1\u001b[39m / \u001b[32m240\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# x, y, z -> Drone position\u001b[39;00m\n\u001b[32m    139\u001b[39m reward, done, terminated, truncated = \u001b[38;5;28mself\u001b[39m.calc_reward()\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# return obs, reward, terminated, truncated, {}\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git_Repos\\Drone-DeepRL\\src\\environments\\EnvRandomReturn.py:210\u001b[39m, in \u001b[36mEnvRandomReturn.get_observation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     pos, _ = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetBasePositionAndOrientation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdrone_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# Include distance to target into obeservation\u001b[39;00m\n\u001b[32m    213\u001b[39m     distance = np.linalg.norm(np.array(pos) - \u001b[38;5;28mself\u001b[39m.current_target)\n",
      "\u001b[31merror\u001b[39m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "while t <= REPLAY_BUFFER.capacity:\n",
    "    state, _info = ENV.reset()# ENV.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        \"\"\" \n",
    "        Only for the pretraining of the WORLDMODEL before entering the main training loop\n",
    "        make the randomness of up and down lesser so we get longer episodes and the model\n",
    "        learns the dependencies between states and its rewards better aswell as logner episodes.\n",
    "        \"\"\"\n",
    "        action: int = random.choice(range(ACTION_SIZE))\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=ACTION_SIZE\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, done, _info = ENV.step(action=action)# ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            done # bool\n",
    "        )\n",
    "       \n",
    "        state = next_state\n",
    "\n",
    "        # Update idx\n",
    "        t += 1\n",
    "\n",
    "        if t % 10_000 == 0:\n",
    "            print(f\"Length REPLAY_BUFFER {t}/{REPLAY_BUFFER.capacity}\")\n",
    "\n",
    "\n",
    "#----Train world model----#\n",
    "for epoch in range(EPOCHS):\n",
    "    #for _ in range(len(REPLAY_BUFFER) // BATCH_SIZE):\n",
    "    (states, actions, rewards, next_states, dones) = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "    # Write params to the DEVICE for GPU support\n",
    "    states = states.to(DEVICE)\n",
    "    actions = actions.to(DEVICE)\n",
    "    rewards = rewards.to(DEVICE)\n",
    "    next_states = next_states.to(DEVICE)\n",
    "    dones = dones.to(DEVICE)\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    pred_states, pred_rewards, pred_dones, hidden_state_real = WORLD_MODEL(\n",
    "        states, actions, hidden_state_real\n",
    "    )\n",
    "    hidden_state_real = tuple([h.detach() for h in hidden_state_real])\n",
    "\n",
    "    # Compute losses\n",
    "    state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "    reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "    done_loss = world_model_loss_fn(pred_dones, dones.float())\n",
    "    total_loss = state_loss + reward_loss + done_loss\n",
    "\n",
    "    print(f\"EPOCH: {epoch}; WORLD_MODEL LOSS: {total_loss}\")\n",
    "\n",
    "    optimizer_world_model.zero_grad()\n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer_world_model.step()\n",
    "\n",
    "    # # Update last_real_hidden_state for LSTM Learning\n",
    "    # last_real_hidden_state = hidden_state_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m action_one_hot: np.ndarray = action_one_hot.numpy()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Step in real environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m next_state, reward, done, _ = ENV.step(action)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Apply to memory of current episode\u001b[39;00m\n\u001b[32m     27\u001b[39m REPLAY_BUFFER.add(\n\u001b[32m     28\u001b[39m     state, \u001b[38;5;66;03m# np.ndarray\u001b[39;00m\n\u001b[32m     29\u001b[39m     action_one_hot, \u001b[38;5;66;03m# np.ndarray\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     done \u001b[38;5;66;03m# bool\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# ENV = EnvRandomReturn()\n",
    "# TODO: Test code on cartpole environment\n",
    "for episode in range(EPISODES):\n",
    "    state, _info = ENV.reset()# Only get observation\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon greedy policy\n",
    "        if random.random() < EPSILON:\n",
    "            action: int = random.choice(range(ACTION_SIZE)) # Exploration\n",
    "        else:\n",
    "            q_values = Q_NETWORK(torch.tensor(state, dtype=torch.float32))\n",
    "            action = torch.argmax(q_values).item() # Exploitation\n",
    "\n",
    "        # Transform action len=1 to one_hot len=6 for WorldModle input\n",
    "        action_one_hot: torch.Tensor = torch.nn.functional.one_hot(\n",
    "            torch.tensor(action, dtype=torch.long), num_classes=ACTION_SIZE\n",
    "        )\n",
    "        # Transform to numpy array for cleare understanding -> Not needed later i think\n",
    "        action_one_hot: np.ndarray = action_one_hot.numpy()\n",
    "\n",
    "        # Step in real environment\n",
    "        next_state, reward, done, _ = ENV.step(action)\n",
    "\n",
    "        # Apply to memory of current episode\n",
    "        REPLAY_BUFFER.add(\n",
    "            state, # np.ndarray\n",
    "            action_one_hot, # np.ndarray\n",
    "            reward, # np.float64\n",
    "            next_state, # np.ndarray\n",
    "            done # bool\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Decay Epsilon\n",
    "    EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    #----------Train WorldModel using real data----------#\n",
    "    if len(REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN and episode % TRAIN_WORLD_MODEL_EVERY == 0:\n",
    "        # Sample real data from REPLAY_BUFFER\n",
    "        (states, actions, rewards, next_states, dones) = REPLAY_BUFFER.sample(BATCH_SIZE)\n",
    "\n",
    "        # Write params to the DEVICE for GPU support\n",
    "        states = states.to(DEVICE)\n",
    "        actions = actions.to(DEVICE)\n",
    "        rewards = rewards.to(DEVICE)\n",
    "        next_states = next_states.to(DEVICE)\n",
    "        dones = dones.to(DEVICE)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        pred_states, pred_rewards, pred_dones, hidden_state_real = WORLD_MODEL(\n",
    "            states, actions, hidden_state_real\n",
    "        )\n",
    "        hidden_state_real = tuple([h.detach() for h in hidden_state_real])\n",
    "\n",
    "        # Compute losses\n",
    "        state_loss = world_model_loss_fn(pred_states, next_states)\n",
    "        reward_loss = world_model_loss_fn(pred_rewards, rewards)\n",
    "        done_loss = world_model_loss_fn(pred_dones, dones.float())\n",
    "        total_loss = state_loss + reward_loss + done_loss\n",
    "\n",
    "        print(f\"EPISODE: {episode}; WORLD_MODEL LOSS: {total_loss}\")\n",
    "\n",
    "        optimizer_world_model.zero_grad()\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer_world_model.step()\n",
    "\n",
    "        # Update last_real_hidden_state for LSTM Learning\n",
    "        last_real_hidden_state = hidden_state_real\n",
    "\n",
    "\n",
    "    # ---- Use World Model as a simulator to train the Q-Network ----\n",
    "    for idx, _ in enumerate(range(SIMULATED_UPDATES)):  # Train Q-network multiple times using synthetic experience\n",
    "        state_sim = torch.tensor(ENV.reset()[0], dtype=torch.float32, device=DEVICE).unsqueeze(0).unsqueeze(0)  # 1 Batch, 1 Seq, 5 elements\n",
    "\n",
    "        # Intitialize hidden state sim from last real hidden state (num_layers, BATCH, hidden_size)\n",
    "        hidden_state_sim = (\n",
    "            last_real_hidden_state[0][:, 0:1, :].detach(),  # Shape: (num_layers, 1, hidden_size)\n",
    "            last_real_hidden_state[1][:, 0:1, :].detach()\n",
    "        )\n",
    "\n",
    "        for _ in range(IMAGINARY_HORIZON):\n",
    "            # Choose action using current Q-network\n",
    "            q_values = Q_NETWORK(state_sim)\n",
    "            action_sim = torch.argmax(q_values, dim=-1).item()\n",
    "\n",
    "            # Convert action to one-hot encoding\n",
    "            action_one_hot_sim = torch.nn.functional.one_hot(\n",
    "                torch.tensor(action_sim, dtype=torch.long, device=DEVICE), num_classes=ACTION_SIZE\n",
    "            ).unsqueeze(0).unsqueeze(0)\n",
    " \n",
    "            # Predict next state, reward, and done using the World Model\n",
    "            predicted_next_state, predicted_reward, predicted_done, hidden_state_sim = WORLD_MODEL(\n",
    "                state_sim, action_one_hot_sim, hidden_state_sim\n",
    "            )\n",
    "\n",
    "            # Store synthetic experience\n",
    "            # Remove dimensionalitys which were needed for LSTM to store in buffer\n",
    "            IMAGINARY_REPLAY_BUFFER.add(\n",
    "                state_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                action_one_hot_sim.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                predicted_reward.squeeze().item(),\n",
    "                predicted_next_state.squeeze(0).squeeze(0).detach().cpu().numpy(),\n",
    "                predicted_done.squeeze().item()\n",
    "            )\n",
    "\n",
    "\n",
    "            # Ensure termination if certain 90%\n",
    "            if predicted_done.item() > 0.9:\n",
    "                break\n",
    "            \n",
    "            state_sim = predicted_next_state.detach()  # Move to predicted next state\n",
    "\n",
    "    # ---- Train Q-Network on synthetic experience ----\n",
    "    if len(IMAGINARY_REPLAY_BUFFER) >= BATCH_SIZE + SEQ_LEN:\n",
    "        # Train from real and synthetic experiences\n",
    "        real_sample_size = int(BATCH_SIZE * 0.5)  # Half from real data\n",
    "        sim_sample_size = BATCH_SIZE - real_sample_size  # Half from simulated\n",
    "\n",
    "        real_experiences = REPLAY_BUFFER.sample(real_sample_size)\n",
    "        simulated_experiences = IMAGINARY_REPLAY_BUFFER.sample(sim_sample_size)\n",
    "\n",
    "        # Combine real and simulated experiences\n",
    "        states_sim = torch.cat([real_experiences[0], simulated_experiences[0]], dim=0).to(DEVICE)\n",
    "        actions_sim = torch.cat([real_experiences[1], simulated_experiences[1]], dim=0).to(DEVICE)\n",
    "        rewards_sim = torch.cat([real_experiences[2], simulated_experiences[2]], dim=0).to(DEVICE)\n",
    "        next_states_sim = torch.cat([real_experiences[3], simulated_experiences[3]], dim=0).to(DEVICE)\n",
    "        dones_sim = torch.cat([real_experiences[4], simulated_experiences[4]], dim=0).to(DEVICE).float()\n",
    "\n",
    "        # Get Q-values\n",
    "        q_values = Q_NETWORK(states_sim)  \n",
    "        actions_sim_indices = actions_sim.argmax(dim=-1).unsqueeze(-1)# .view(64, 32, 1)  # Shape [64, 32, 1]\n",
    "\n",
    "        # Now we gather the Q-values corresponding to the actions\n",
    "        q_value = q_values.gather(2, actions_sim_indices)\n",
    "        \n",
    "        # Compute target using Bellman equation\n",
    "        next_q_values = Q_NETWORK(next_states_sim).max(-1)[0].unsqueeze(-1)  # Max Q-value for next state\n",
    "        target = rewards_sim + GAMMA * next_q_values * (1 - dones_sim)  # Bellman update\n",
    "\n",
    "        # Compute loss\n",
    "        loss_q = torch.nn.MSELoss()(q_value, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_q_network.zero_grad()\n",
    "        loss_q.backward()\n",
    "        optimizer_q_network.step()\n",
    "\n",
    "        print(f\"Q-Network Loss: {loss_q.item():.4f}\")\n",
    "\n",
    "\n",
    "# Save models\n",
    "torch.save(WORLD_MODEL.state_dict(), \"world_model.pth\")\n",
    "torch.save(Q_NETWORK.state_dict(), \"dqn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
